{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that processes the comments and returns clean_comments\n",
    "\n",
    "def text_preprocessing(data):\n",
    "\n",
    "    \n",
    "\n",
    "    #convert to lowercase\n",
    "    data['clean_comments'] = data['comments'].str.lower()\n",
    "\n",
    "    #removing empty columns\n",
    "    data['clean_comments'].replace('', np.nan, inplace=True)\n",
    "    data.dropna(subset=['clean_comments'], inplace=True)\n",
    "\n",
    "    #removing punctuations\n",
    "    punctuation = string.punctuation\n",
    "    data['clean_comments'] = data['clean_comments'].astype(str)\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: x.translate(str.maketrans('','',punctuation)))\n",
    "\n",
    "    #removing stopwords\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: \" \".join([w for w in x.split() if w not in STOPWORDS]))\n",
    "\n",
    "    #remove non english words\n",
    "    WORDS = set(words.words())\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: \" \".join(w for w in x.split() if w in WORDS))\n",
    "\n",
    "    #removing special characters\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub('[^a-zA-Z0-9]', ' ', x))\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub('\\s+', ' ', x))\n",
    "\n",
    "    #removing urls and html tags\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub(r'https?://\\S+www\\.\\S+', ' ', x))\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub(r'<.*?>', ' ', x))\n",
    "\n",
    "    #stemization\n",
    "    #ps = PorterStemmer()\n",
    "    #data['clean_comments'] = data['clean_comments'].apply(lambda x: \" \".join([ps.stem(w) for w in x.split()]))\n",
    "\n",
    "    #lemmatization and pos tagging\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\":wordnet.NOUN,\"V\":wordnet.VERB,\"J\":wordnet.ADJ,\"R\":wordnet.ADV}\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: \" \".join([lemmatizer.lemmatize(w,wordnet_map.get(pos[0],wordnet.NOUN)) for w, pos in pos_tag(x.split())]))\n",
    "    \n",
    "    #removing empty columns\n",
    "    data['clean_comments'].replace('', np.nan, inplace=True)\n",
    "    data.dropna(subset=['clean_comments'], inplace=True)\n",
    "\n",
    "    #removing null value\n",
    "    #data['clean_comments'].dropna(how = any, axis = 0)\n",
    "\n",
    "    #word tokanize\n",
    "    #data['clean_comments'] = data['clean_comments'].apply(lambda x: word_tokenize(x))\n",
    "    #data\n",
    "    \n",
    "    #using globle variable i and modifying it after saving into csv\n",
    "    global y\n",
    "    data.to_csv(\"C:\\\\Users\\\\11ani\\\\OneDrive\\\\Desktop\\\\project 7th sem code\\\\clean_comments\\\\\"+str(y)+\"clean.csv\")\n",
    "    y = y+1\n",
    "    print(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comments  sentiments  \\\n",
      "0   hello sir i need to scrap a project similar to...         NaN   \n",
      "2   i wanted to ask if there is way to only select...         NaN   \n",
      "3                                     Thanks a lot :D         NaN   \n",
      "4   How do I scrape the titles of VIDEOS instead o...         NaN   \n",
      "5   I want to create dataset for sentiment analysi...         NaN   \n",
      "6   can you tell me what drive have you used, the ...         NaN   \n",
      "7      How to scrape data from age restricted videos?         NaN   \n",
      "8     How do this task for 10000 channels in one shot         NaN   \n",
      "9                             plz share the code link         NaN   \n",
      "10  Youtube channels ki email kese scrape karen?\\n...         NaN   \n",
      "11  arre bhai mera ho gya mene crome drive use kia...         NaN   \n",
      "12               multiple links ko kesy scrap karain?         NaN   \n",
      "13  Bhai python me error aa raha hai aap apni emai...         NaN   \n",
      "14  bro mera chrome driver d location maa woo get ...         NaN   \n",
      "15                                  I cant understand         NaN   \n",
      "18                                                 Hi         NaN   \n",
      "19  how can you write the title in english and spe...         NaN   \n",
      "\n",
      "                                       clean_comments  \n",
      "0   hello sir need scrap project similar base need...  \n",
      "2   ask way select channel specific country like w...  \n",
      "3                                          thanks lot  \n",
      "4                                      scrape instead  \n",
      "5   want create sentiment analysis top view excel ...  \n",
      "6               tell drive use voice clear part video  \n",
      "7                            scrape data age restrict  \n",
      "8                                       task one shot  \n",
      "9                                     share code link  \n",
      "10                      scrape please reply fast need  \n",
      "11  ho crome drive use data save ha jo se extract ...  \n",
      "12                             multiple link ko scrap  \n",
      "13                           python error aa pic help  \n",
      "14                     chrome driver location woo get  \n",
      "15                                    cant understand  \n",
      "18                                                 hi  \n",
      "19                         write title speak language  \n",
      "                                             comments  sentiments  \\\n",
      "0   Hey, nice to meet you! I just found your chann...         NaN   \n",
      "1   this is extremely helpful mate! \\nkeep making ...         NaN   \n",
      "2   Thanks a lot for the tutorial! It helps a lot!...         NaN   \n",
      "3   Love you, my respected brother, although the m...         NaN   \n",
      "4   i am so  happy after watching this video. Than...         NaN   \n",
      "5        Hey sir, how do you take 200 comments first?         NaN   \n",
      "6   it works Thanks but it is very slow. Can you d...         NaN   \n",
      "7                         Thank you brother for video         NaN   \n",
      "8   Hi, is it legal to scrap youtube comments ? Th...         NaN   \n",
      "9        can't we just use pytube to print the title?         NaN   \n",
      "10        Is the code of this Video available? thanks         NaN   \n",
      "11           Can we scrape emails from these comments         NaN   \n",
      "12                            Where you run this code         NaN   \n",
      "13  It is sayinf that >= does not work with int an...         NaN   \n",
      "14                                I DO NOT UNDERSTAND         NaN   \n",
      "15  Bruh, can you learn english before doing video...         NaN   \n",
      "16  I am getting this key error: \\nKeyError: <_SSL...         NaN   \n",
      "\n",
      "                                       clean_comments  \n",
      "0   hey nice meet find channel love like clear det...  \n",
      "1   extremely helpful mate keep make informative v...  \n",
      "2   thanks lot tutorial lot definitely way efficie...  \n",
      "3   love brother although method get lot video kee...  \n",
      "4                      happy watch video much content  \n",
      "5                                  hey sir take first  \n",
      "6                                work thanks slow via  \n",
      "7                                 thank brother video  \n",
      "8                               hi legal scrap thanks  \n",
      "9                                cant use print title  \n",
      "10                        code video available thanks  \n",
      "11                                             scrape  \n",
      "12                                           run code  \n",
      "13                                 work non type help  \n",
      "14                                         understand  \n",
      "15                                     learn language  \n",
      "16                          get key error please help  \n",
      "                                              comments  sentiments  \\\n",
      "0    Thanks to FCC for posting my Web Scraping cour...         NaN   \n",
      "1    I thought this skill would be way above my lev...         NaN   \n",
      "2    This is definitely one of the best hours I spe...         NaN   \n",
      "3    this is what I call quality content. Very logi...         NaN   \n",
      "4    I just have to say, this is the best webscrapi...         NaN   \n",
      "..                                                 ...         ...   \n",
      "615                       Literally a waste of my time         NaN   \n",
      "616                              Borat teaches Python!         NaN   \n",
      "617                                                  L         NaN   \n",
      "618           I stopped watching because of the accent         NaN   \n",
      "619  Please learn to pronounce 'THE' !  Gave up wat...         NaN   \n",
      "\n",
      "                                        clean_comments  \n",
      "0    thanks post web scrap course huge achievement ...  \n",
      "1    thought skill would way level still student mi...  \n",
      "2    definitely one best spent quality content free...  \n",
      "3    call quality content logically instruct thank ...  \n",
      "4                      say best course see far well do  \n",
      "..                                                 ...  \n",
      "615                               literally waste time  \n",
      "616                                             python  \n",
      "617                                                  l  \n",
      "618                                  stop watch accent  \n",
      "619        please learn pronounce give watch video min  \n",
      "\n",
      "[604 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "y = 0\n",
    "\n",
    "for i in range(3):\n",
    "    data = pd.read_csv(\"C:\\\\Users\\\\11ani\\\\OneDrive\\\\Desktop\\\\project 7th sem code\\\\raw_comments\\\\\"+str(i)+\"raw.csv\", index_col=[0])\n",
    "    text_preprocessing(data)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b9de5f633baef838eda09087c377b407887e2414fdad0aa392eeb39aea32887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

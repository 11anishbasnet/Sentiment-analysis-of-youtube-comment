{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing to download youtube videos\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Importing Pandas to create DataFrame\n",
    "import pandas as pd\n",
    "# Importing numpy\n",
    "import numpy as np\n",
    "\n",
    "#importing for plot data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to scrap youtube commentr from video and returns as dataframe df\n",
    "def ScrapComment(url):\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=option)\n",
    "    driver.get(url)\n",
    "    time.sleep(10)\n",
    "    prev_h = 0\n",
    "    while True:\n",
    "        height = driver.execute_script(\"\"\"\n",
    "                function getActualHeight() {\n",
    "                    return Math.max(\n",
    "                        Math.max(document.body.scrollHeight, document.documentElement.scrollHeight),\n",
    "                        Math.max(document.body.offsetHeight, document.documentElement.offsetHeight),\n",
    "                        Math.max(document.body.clientHeight, document.documentElement.clientHeight)\n",
    "                    );\n",
    "                }\n",
    "                return getActualHeight();\n",
    "            \"\"\")\n",
    "        driver.execute_script(f\"window.scrollTo({prev_h},{prev_h + 200})\")\n",
    "        # fix the time sleep value according to your network connection\n",
    "        time.sleep(1)\n",
    "        prev_h +=300  \n",
    "        if prev_h >= height:\n",
    "            break\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "    title_text_div = soup.select_one('#container h1')\n",
    "    title = title_text_div and title_text_div.text\n",
    "    comment_div = soup.select(\"#content #content-text\")\n",
    "    comment_list = [x.text for x in comment_div]\n",
    "    print(title, comment_list)\n",
    "    df =pd.DataFrame(comment_list,columns=['comments'])\n",
    "    df[\"sentiments\"] = \"\"\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing string,nltk and re for text preprocessing\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "#function that processes the comments and returns clean_comments as df\n",
    "\n",
    "def text_preprocessing(data):   \n",
    "\n",
    "    #convert to lowercase\n",
    "    data['clean_comments'] = data['comments'].str.lower()\n",
    "\n",
    "    #removing empty columns\n",
    "    data['clean_comments'].replace('', np.nan, inplace=True)\n",
    "    data.dropna(subset=['clean_comments'], inplace=True)\n",
    "\n",
    "    #removing urls and html tags\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub(r'https?://\\S+www\\.\\S+', ' ', x))\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub(r'<.*?>', ' ', x))\n",
    "\n",
    "    #removing punctuations\n",
    "    punctuation = string.punctuation\n",
    "    data['clean_comments'] = data['clean_comments'].astype(str)\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: x.translate(str.maketrans('','',punctuation)))\n",
    "\n",
    "    #removing stopwords\n",
    "    STOPWORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',\n",
    "       \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself',\n",
    "       'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her',\n",
    "       'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them',\n",
    "       'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',\n",
    "       'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
    "       'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',\n",
    "       'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and',\n",
    "       'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at',\n",
    "       'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "       'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
    "       'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',\n",
    "       'again', 'further', 'then', 'once', 'here', 'there', 'when',\n",
    "       'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "       'most', 'other', 'some', 'such', 'nor', 'only', 'own', 'same',\n",
    "       'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just',\n",
    "       'don', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're',\n",
    "       've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'doesn',\n",
    "       \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "       'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    "       'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "       'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn',\n",
    "       \"wouldn't\",'another','other']\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: \" \".join([w for w in x.split() if w not in STOPWORDS]))\n",
    "\n",
    "    #removing special characters\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub('[^a-zA-Z0-9]', ' ', x))\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub('\\s+', ' ', x))\n",
    "\n",
    "    #stemization\n",
    "    #ps = PorterStemmer()\n",
    "    #data['clean_comments'] = data['clean_comments'].apply(lambda x: \" \".join([ps.stem(w) for w in x.split()]))\n",
    "\n",
    "    #lemmatization and pos tagging\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\":wordnet.NOUN,\"V\":wordnet.VERB,\"J\":wordnet.ADJ,\"R\":wordnet.ADV}\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: \" \".join([lemmatizer.lemmatize(w,wordnet_map.get(pos[0],wordnet.NOUN)) for w, pos in pos_tag(x.split())]))\n",
    "    \n",
    "    #removing empty columns\n",
    "    data['clean_comments'].replace('', np.nan, inplace=True)\n",
    "    data.dropna(subset=['clean_comments'], inplace=True)\n",
    "    \n",
    "    return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokanization function to tokanize sentence to words\n",
    "def sentence_to_words(sentence):\n",
    "\n",
    "    l = sentence.split()  # split sentence into individual word(tokanize)\n",
    "    p = ''\n",
    "    word_list = []\n",
    "\n",
    "    for word in l:\n",
    "\n",
    "        p = ''\n",
    "\n",
    "        for letter in word:\n",
    "\n",
    "            if ord(letter) >= 67 and ord(letter) <= 122:\n",
    "                p = p + letter\n",
    "        word_list.append(p)\n",
    "\n",
    "    return word_list  # return the word list of the input comments\n",
    "\n",
    "def naive_bayes_predict(X, bag, prior_pos, prior_neg):\n",
    "    Y = []\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        k_pos = 1\n",
    "        k_neg = 1\n",
    "        p = sentence_to_words(X[i])\n",
    "\n",
    "        for k in range(len(bag)):\n",
    "\n",
    "            for word in p:\n",
    "\n",
    "                if word == bag['index'][k]:\n",
    "                    k_pos = k_pos * bag['sent=positive'][k] #product of likelihood prob given the word is present in vocabulary \n",
    "                    k_neg = k_neg * bag['sent=negative'][k]\n",
    "\n",
    "        nb = [prior_neg * k_neg, prior_pos * k_pos] # multiply each likelihood prob with the prior prob\n",
    "        Y.append(np.argmax(nb))\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[39m#input url\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mEnter your url:\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m     \u001b[39m#scraping comment from url\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     df \u001b[39m=\u001b[39m ScrapComment(url)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py:1175\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1172\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1173\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[1;32m-> 1175\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[0;32m   1176\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[0;32m   1177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1179\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1180\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel\\kernelbase.py:1217\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1216\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m   1218\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1219\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #input url\n",
    "    url = input(\"Enter your url:\")\n",
    "    \n",
    "    #scraping comment from url\n",
    "    df = ScrapComment(url)\n",
    "    #df = pd.read_csv(\"test.csv\")\n",
    "    \n",
    "    #pre-processing of the text\n",
    "    df = text_preprocessing(df)\n",
    "    print (df)\n",
    "    \n",
    "    Z = df['clean_comments']\n",
    "    Z = Z.reset_index(drop=True)\n",
    "\n",
    "    #opening bag of words we have created and finding P(c) as prior_pos and P(n) as prior_neg\n",
    "    bag = pd.read_csv(\"bag.csv\", index_col=[0])\n",
    "    prior = pd.read_csv(\"prior.csv\", index_col=[0])\n",
    "\n",
    "    prior_pos = prior.iloc[0]['prior']  # prior probability for  class\n",
    "    prior_neg = prior.iloc[1]['prior']  #prior probability for class \n",
    "\n",
    "    #print(prior_pos,prior_neg)\n",
    "\n",
    "    z_predicted = naive_bayes_predict(Z, bag, prior_pos, prior_neg)\n",
    "    #z_predicted\n",
    "\n",
    "    #saving predicted sentiments into dataframe and to csv\n",
    "    df['sentiments'] = z_predicted\n",
    "    df.to_csv(\"C:\\\\Users\\\\11ani\\\\OneDrive\\Desktop\\\\project 7th sem code\\\\predected_comments\\\\\"+\"temp.csv\")\n",
    "\n",
    "    #analysis of comments\n",
    "    length = len(df)  # finding out length of the datframe\n",
    "    pos_count = len(df[df['sentiments'] == 1])  # counting positive_sentiments\n",
    "    neg_count = len(df[df['sentiments'] == 0])  # counting negative_sentiments\n",
    "    print ('length=',length,'\\npos_count',pos_count,'\\nneg_count',neg_count,)\n",
    "\n",
    "    y = np.array([pos_count,neg_count])\n",
    "    mylabels = [\"positive comments\", \"negative comments\"]\n",
    "    print(df)\n",
    "    plt.pie(y, labels= mylabels,  autopct='%1.1f%%')\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b9de5f633baef838eda09087c377b407887e2414fdad0aa392eeb39aea32887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

,comments,sentiments
0,"Thanks to FCC for posting my Web Scraping course. 
This is a huge achievement for me as a consistent learner from this channel :)
I hope to see you all around in my future uploads!",
1,"I thought this skill would be way above my level as i'm still a student mid way through my under-graduate , but you guys made it look like a child's play. I’m actually able to do web scraping after watching your video. Well done FCC team , good job boys.",
2,This is definitely one of the best hours I spent on YouTube. Quality content for free. We don't deserve this...,
3,this is what I call quality content. Very logically presented and instructed. Thank you very much for contributing to my future's success in Python.,
4,"I just have to say, this is the best webscraping course I have seen so far. Very well done !",
5,The amount of hard work and dedication you guys put to create such free content is just Amazing!!!,
6,Just a suggestion @37:09 it is better to use text.strip() method to remove unnecessary space from the beginning and the end. replace removes spaces from the company name as well.,
7,"This was amazing, I didn't know web scraping could be so easy",
8,"Just wanted to say thank you! One of the best tutorials that I have seen on webscraping, and very easy to understand and follow. Great Job!",
9,"As someone rather new to coding, I've tried several web scraping tutorials but none of them ""clicked"" for me. Your tutorial however was fantastic. So easy to follow along, it was incredible!!! Thanks you!!",
10,Great video man - I’m so excited that I was able to make a basic web scraper on my own after watching this  (but needs a ton of refinement! Haha). Thanks for a great video!,
11,This was an amazing tutorial! You have a gift for teaching!,
12,"Thank you! This was a great introduction for a total beginner to scraping. By the end of it, I was able to use what you taught me on a different website, and I'm starting a scraping project now with what I just learned.",
13,"Excellent tutorial for a total web scraping novice, after watching this I'm already able to pull data from several different websites for my own use case. Knowing very little html coming in, I already feel like I know enough to get to some useful information to analyze offline. Very easy to follow and a great flow by the presenter Jim. Thank you.",
14,"Awesome video tutorial on how to perform web scraping with Python using the Beautiful Soup library. Just curious does Python + Beautiful Soup can cover all web-scraping cases? 
Can Python + Scrapy + Selenium  cover more difficult cases?",
15,"Glad to finish the entire discussion. I am a 3rd year Computer Science student and this helped me out A LOT! 
Thanks again for the wonderful content.",
16,This is the best Python educational video I've watched. The difficulty progression and pacing were perfect.,
17,"Fantastic! This video has been super helpful! If you have a little programming background, you can even fast forward through the video and just get your own piece of code done in half an hour! Very well done!",
18,"This is a fantastic tutorial, probably one of the best I have ever come across. I came into this tutorial with 0 knowledge on web scraping, and came out of it with much more confidence. Excellent work!",
19,Just 15 minutes into the course and I'm already feeling like a web scraping pro . Thank you so much,
20,"Great explanation, easy to follow. Just starting out with the whole scraping thing and here you get some good tips and concepts explained. Well done!! Thank you",
21,"Thanks so much for the video , I was always curious how web scraping was done and you made it seem like it was a walk in the park , and it essentially was !",
22,"I see some comments about people receive none from scraping responses, unfortunately it might be expected from websites that are dynamically changed and being updated, there is a great chance that the website had some updates in this 4-month span, and then it affects your results. Try to scrape differently by inspecting to the elements like I show in the website user interface :)",
23,Absolutely brilliant course. An excellent introduction to web scraping and how python programmers solve problems. Well done.,
24,"The explanation is quite simple and clear...
I love the way he explain little little things in the whole topic, so that every person can easily understand.
",
25,"Thanks for the great tutorials as always from FCC and great work @JimShapedCoding. I  have learned a few tricks here for sure, one thing i have noticed there was that the code assumes structure of the HTML elements to be consistent and this will not always be the case with other websites, so its good to do some checks and handle some exceptions incase the element required is not found :)",
26,"Great video and very informative. It was very enjoyable, but I was just wondering what's the difference between .find(""div"") and .div? Thanks for making this video.",
27,Great explanation. It would be wonderful if we could have a tutorial on scrapy as well,
28,"This is so cool! I just finished the whole video and was able to follow through. It's amazing to know what Python can do.

Thank you!",
29,I really loved your content! Thanks for helping me with paced and informative explanations!!!! Looking forward to many tutorials from you!!,
30,"The word 'Web Scrapping' was like a nightmare for me before I saw this video.
Everytime I saw any web scrapping job in the job portals I was assuming that it's beyond my scope.
But now I am confident enough to answer questions related to this after going through this video and doing some hands on as well. 

Thanks to Jim for such a beautiful explanation and thanks to FCC too for uploading the tutorial video. ",
31,"I have to give a big thumb up for this tutorial!!!! Very well explained, easy to understand and great pace! I hope to see more tutorials from this teacher. Thanks!!!",
32,A one hour video for bs4 tutorial explaining everything in such clarity and detail. This dude is nuts! Just take my like and subscribe. Just take it all.,
33,"Wow, what a quality content, systematic and well taught. Thank you.",
34,"I would like to thank you. You are a world-class tech teacher as you help me to decide and start my focus on web scrapping. Your way of explanation is so compelling.
Please add other web scrapping tips on with your You Tube channel.",
35,"really awesome video.... I don't think i have ever commented on a video tutorial before and i have watched a ton of them... but this is by far the best video tutorial i have ever watched.... thanks FCC, thanks Jim",
36,"Great video! I was struggling with extracting the data from the website, but you did a great job explaining!",
37,Beautiful! I would definitely recommend for anyone starting out with web scraping.,
38,Thank you for the well paced and informative tutorial.  Best web scraping intro I've found.,
39,Your teaching skills are simple and incredible! Thank you,
40,Great Video!!! I've wanted to learn web scraping for a while and this video just saved me so many hours. Just awesome!,
41,"Great video. Would love similar web scraping videos for Twitter, Reddit, Amazon, and Instagram.",
42,"Voww voww voww. I loved this tutorial. Followed from beginning to end. I liked it that you put the filtering condition while scraping the data, extra features and all. Just awesome. This is a must watch for all beginners. Huge thanks to both Jim and FCC",
43,"Great Tutorial. It was very informative. Way better than paid courses. Thumbs up man!!!!!!!
I have subscribed to your channel.",
44,"This course is mindblowing...
first, the pace is excellent...no bullshiting, straight to the point, clear and concise,
secondly, very detailed... I was able to scrap a real estate website for an analysis i wanted to do

I have found web scrapping quite intimidating, but man this course made it so easy. You are a great teacher. Im short of words",
45,Excellent quality content video and great teacher. Thank you Jim and free code camp,
46,"Having requirements.txt file to quickly set up my own virtual environment would be very convenient. Otherwise excellent tutorial! I liked the pace, the logic, the structure, the speech of the presentation.",
47,Thank you so much. Leant web-scrapping and completed my assignment in just a few hours.,
48,"Really love your clear explanation, the word that you pick is comprehensible. Keep up the good work!!!",
49,"Great tutorial! I would add that you can name the file with a unique timestamp, because after 10 minutes the files would get overwritten if you are just using the index",
50,"Thank you so much for the wonderful tutorial, you have great skills with Python as well as in teaching. It was a great learning experience !",
51,57:40 simply store the input of the user in a list variable and using the for loop to check if any of the input is not in the skills variable,
52,Great video. One quick question although. At the moment we are searching the hard coded values. Is it possible to scrape the information based on the matching pattern?,
53,One of the best tutorials for beginners...wow.I just played in 0.9x speed and it was very clear.Thank you to this guy and the channel.,
54,"Great video! Great tutorial! Great teacher! You have explained it very clearly, thank you!",
55,"Amazing tutorial! Super easy to understand tutorial! 
Thank you, Jim!",
56,"Excellent content...  structured and explained in a way easy to understand. I'm not native english spoken and still learning both english as python. So, thank so much. 

Can you give us some strategies to handle pagination ? Selenium maybe or other one ?",
57,"Thank you so much for such a short and very informative video!
It helped me a lot!",
58,Great contribution from FCC team!. After a long time I could reach the right content for learning web scrapping and loving it. Before  that I have checked a lot of web scrapping related tutorial which made me frustrated! Love you man....,
59,This surely is one of the best tutorial on web scraping. I was hoping to see how to iterate for listing spread across multiple pages.,
60,"Thank you for such a great video! I have question as you said at the beginning that we will scrap only the first page, is it possible to create a prg that goes through all the existing pages (supposing that we have a limited number of pages,  15 for example). Would it be possible?",
61,"An hour, well spent. Good job and thank you!",
62,"such a great course  to walk me through the whole logic about web scraping, thx a lot !",
63,"Amazing work! easy web scraping right off the bat, thanks also for the effort in putting words to english.",
64,"This is some quality content, it is much appreciated!",
65,"Outstanding explanation and learned a lot. Thanks, Jim for your great efforts. I love your PyCharm code editor theme. Please tell me which theme are you using in this content.",
66,"Very tight presentation for beginners, and greybeards like me can skip around, iterate, and then see how you chose to do it. 5 ",
67,A great video thanks!!! But can you add how you do pagination as well or if you have another video for that for when the list of jobs go to multiple pages?,
68,"Thanks a lot, your tutorial is quite clear and right to the point.",
69,Thank you so much for taking the time to make this video. I appreciate you,
70,"Very great video and explanation! However, is it possible to scrape through all pages instead of just page 1?",
71,"I absolutely love this channel, thanks for teaching us about coding",
72,"Excellent tutorial, very well explained. Thank you!",
73,"this is exactly what I was looking for, thanks a lot",
74,Best 1 hour I spent on youtube in a while. Thank you for this FreeCodeCamp!,
75,"Hi Jimshaped, thank you for this tutorial, it's really clear and helpfull.
Sincerly, the code isn't so long but is it possible to copy it or download it somewhere? thank you :-)",
76,Very nice tutorial. Have you done one on pagination?,
77,"A great tutorial, well explained, well organized, high recommended for anyone who want to understand webscraping using python.",
78,Best Web Scraping video out there for beginners.,
79,"This is really a wonderful tutorial, thank you!",
80,Very clear the process step by step. Wonderful educational material ! Thank you so much !,
81,Great tutorial! Very easy to follow along.,
82,"this video is incredible useful and amazingly explained, thank you very much for all the info <3",
83,very beautiful tutorial !  Thank you very much.  You taught me the basics of web scraping in approx. 68 minutes,
84,"Great explanation, clearly showing step by step approach. Appreciate it !!!!!!!!",
85,"Great course.
If you could post a web scrapping with Selenium course it would be nice.",
86,I really enjoyed this tutorial... So detailed and easy to understand.,
87,"Hi Jim, very cool video! I tried to run a code to parse some data from a website, but I am getting an error when using the "".text"" filter it return an error when the resulting valeu is None. It occurs sometimes when a certain tag exists in one container but not in the other. I should probably write a function right?",
88,To the point content with real world scenarios. The only drawbacks would be that this tutorial was missing an example of filtering html element by multiple attributes and some sites have infinite scrolling which displays more content as you scroll down.,
89,"super cool content, great quality, sound, resolution, everything. Very easy to understand also. Thank you so much.",
90,This is my third course that i finished with Jim about Python !!! Thank you,
91,Thank you! One step further would be to add Selenium to the project so that you can navigate also to the next page... I would be very interested in that (how to navigate with selenium and the parse the results after an action with beautifulsup),
92,"Nicely done, sir! Thank you!",
93,"As a proxy company, we know that web scraping is a valuable tool for many businesses. This video does a great job of breaking down the concepts and showing how to use the Beautiful Soup library in Python. I highly recommend it.",
94,"Great video, thank you for the walk-through!",
95,"I have already tried different videos and sources for this. Not all had much detailed knowledge. I really appreciate how he explained all the necessary steps. Tips were really great. Thank you for the video,",
96,What a masterclass!! Thanks!!,
97,"I love this series ,   thank you",
98,"This tutorial was quite great!
Super job dude.",
99,Great Explanation . Great  tutor . Explained each and everything in very simple and logical way . You deserve my salute for this lecture SIR !,
100,Thank You very much. I spent my day watching this. You guys are absolutely great.,
101,"Instead of using .replace(), you can also use .strip() to eliminate white space from both sides :D",
102,"for 37:09, you could also just use the .strip() function",
103,Thank you for posting! Great video & very insightful. Anyone running into issues with lxml can use html.parser as the second parameter to BeautifulSoup() and it works the same.,
104,You are one of the best educator that I have ever seen.I am impatient to see your other videos.Thanks for everthing,
105,"just finished the video and followed all along the series. it all did great!
thanks sir.",
106,"Excellent video, so well explained. 
Thank you!",
107,This is a very thorough course. I know a little something about webscraping but I'm enjoying this nonetheless. Thanks!,
108,"Great and wonderful lecture!!! Thanks a lot, JIM",
109,Well done! Great instructor ,
110,"Great video, very clear and interresting, thank you so much",
111,"this is an excellent course, very useful. and I want to ask, how is the information data that has been obtained exported into excel in python?. Thank you sir",
112,This is definitely one of the best hours I spent on YouTube. :),
113,Absolutely fantastic! Thank you very much!!,
114,Amazing!! Thankyou so much for this video. i wish i could  have found this video sooner :),
115,"Actually interesting course, thank you :)",
116,"Hi there! Thanks for creating this video, it's great! This video is aiding me to learn extracting data from websites. Though I have a question that how to extract data from a website where our interested information is present in the middle of the page. Furthermore, all sections use the same kind a tag (say 'div') that highlight sections of information(including our section of extraction) and these tags have same values for 'id' and 'class' variable. Also, how to only extract that interested data and not the whole page with these following conditions and append it to a csv file. I am working on a project where we require to extract data from journal websites for their manuscript requirements. Your comment on this would be great!",
117,"This is a good tutorial thanks, very well explained  :)",
118,This is one of the very good courses to start with learning web scrapping,
119,"Brilliant, excellent content, thanks a lot... I also wonder about how we parse multiple pages on website at the same time.",
120,cant thank u enough for great content,
121,"Great course! I can't get over how he says ""beautiful soup"" it's wonderful",
122,Anything related to Programming and CS can be learn here easily. Great videos,
123,A great tutorial. Thank you very much for this :D,
124,Well explained and useful. Thanks!,
125,What an exceptional video. Thank you so much /\,
126,Awesome addition to already great content!,
127,The world doesn't deserve this channel. I love you guys. ,
128,"Thanks for this, the lecture is really quite educative.",
129,That helped a lot. Thank you Jim!,
130,What about websites that use Cloudflare? And thank you very much for the small and informative course.,
131,Thanks so much for this... I was sailing in a sea of confusion until I found this... and to imagine its ALL free? like just amazing...,
132,"wow this is a very good free material and it just cover up more than what ""Automate the Boring stuff with Python"" book taught me. this guy just get to the point and did not complicate anything.",
133,Better than my online course that I purchased.,
134,Very useful information in a very easy to understand style. Thank you.,
135,Thank you for this amazing tutorial. Very cool use factor.,
136,Thank you for this useful crash course. Finally I could retrieve pdf files from my personal portfolio.,
137,really good tutorial & easy to understand for someone who knows html/css,
138,Excellent course. Thanks Jim.,
139,"I think I took note of everything I needed to know, thanks!",
140,"Thank your for the clear and precise video on web scraping. I watched your video to refresh my memory.

Btw I must ask, did you update pip? :)",
141,Masterclass! i learned a lot,
142,Thank you so much! This has made looking for new jobs so much easier.,
143,"I rarely comment, but thank you, brother! Amazing job. + Subscribed to you as well, now",
144,"wonderful tutorial ,thank you so much",
145,How do we avoid the 403 Forbidden and the cookies when scrapping ? Thank you for the videos !,
146,It was a great tutorial and informative Thanks for making this code Sir,
147,"Great class.
Keep up the good work.

Thank You,
Natasha Samuel",
148,thanks jim and FCC for this amazing crash course!,
149,"Thank you so much for the video, it's really good!!",
150,"very well done, thnx mate for the informative stuff",
151,Very nice course. The best web scraping course ever. thank you very much.,
152,Very good tutorial. Thanks!,
153,amazing crash course thanks a lot :),
154,This was a fun experience. Thanks!,
155,Great Tutorial and very well explained ..,
156,Got a clear understanding of everything! Thanks @JimShapedCoding !,
157,"Great tutorial, I think you forgot to mention one major issue: how to scrape in multiple pages, for now this script can scrape only one page and nobody is going to make all that for only a page.",
158,"I've tried the web scraping when i am in college but ii already forgotten on how to do it. As i watching the video, my knowledge came back. Thank you. Very good explanation.",
159,Thank you for such a good tutorial.,
160,What if we use stripe() method to remove only leading and trailing whitespaces? That won't hurt inner space or original texts,
161,"Thank you, followed all the tutorial doing my custom script on another website.

I have a question though. How do we save all that info in just one file per script run?",
162,You have no idea how much this video helped me T_T thank you!,
163,Amazing video. I learnt a lot.,
164,Perfect tutorial. Thank you very much,
165,Thanks for this tutorial. Helped me write my next little project. Easier to parse jobs.,
166,"If you are having problems with error 403, try add an user-agent to your request",
167,You should definitely do a course on GO Web programming!,
168,"you made it very easy for me i just scrapped leetcode after watching your videos
thanks a lot for such a great quality education for free",
169,Great tutorial. Question - What if I want to save all the jobs in one file instead of separate files? Thank you in advance.,
170,Thanks a lot for this amazing material!!,
171,Please post few more videos on web scraping. This video is really very helpful. I need a video to scrape the data from pdf file in a  sequence whatever we need  through python,
172,"Thanks for the video, helped me a lot ! ",
173,"Truely amazing content. Though I am running into a problem. My output is coming in lists even with the same codes. So the code 
if unfamiliar_skill not in skills:
is not able to filter out the skill I input. Any solution pls.",
174,Thank you #FCC this is vedio was  very much helpful for me to start my first project after completing python basics...And thank you bro for excellence teaching i understood we well about this web scraping and i will be read to scrap any websites after this video ,
175,"Thanks so much for this tutorial. As someone who is just starting out with web scraping, this has been super helpful. I  attempted your steps and was able to perform most of it. However, I constantly see an error ""'NoneType' object has no attribute 'text'"". The error points to this line ""posting_time = job.find('span', class_ = 'posting-time').text"". Would be great if you or anyone else could help with this.",
176,"Very useful, thanks!",
177,Awesome video. Can you please throw some light on ip rotation proxy handled in web scrapping,
178,This is excellent. Could you tell me how to scrap a website that using Data table js?,
179,"Good video 
I understand all the basics of web scraping using beautiful soup",
180,"this is a great video to study. thank you for this video. But I have a question if I want to scrap the data jobs into an excel file or into database such as SQL Server,  instead of a notepad. Is it ok? Can somebody show me a way to do that if it is possible? 
Thank you",
181,"Hi, what is the tool you are using to read the html code?",
182,It is a nice intro to scrapping for beginners.,
183,"Amazing tutorial, really recommended",
184,This is exactly what i was looking for! tks so much Jim!,
185,"Thanks, bro it helped me a lot in understanding the basic concept
Subscribed to your channel",
186,Can you scrape websites that use template engines like handlebars? If yes kindly give me some pointers or methods. Thx a lot everyone for your time and help,
187,Thanks for the content!!! Really appreciated!,
188,"Link to the home.html file: https://github.com/jimdevops19/codesnippets/tree/main/Python%20Web%20Scraping/01%20-%20Scraping%20Basics

Alternatively if you want to be safer and you’re worried about that link, go to his official website and navigate to get to the same website:
http://www.jimshapedcoding.com/",
189,"Awesome!
Thanks a lot...it was very helpful
Could you pls tell how to scrape multiple web pages?",
190,Great tutorial ,
191,Thanks!  What can we do to signal if the website changes and the code isn't working properly?,
192,"Like literally the best youtube video ever made. It is so good!
Love you guys!",
193,37:25  we can also use strip function instead of replace,
194,"Great video, thanks a lot for it.  
One observation though, I think the method strip() applies better to this case , rather than replace(' ','').  

For example: 

company_name = job.find('h3', class_='joblist-comp-name').text.strip()

Cheers",
195,you teaching skills are another level really thanks alot,
196,This was an amazing introduction. Thank you! You have a new sub JimShapedCoding.,
197,This is my third course that i finished with Jim about Python !!! Thank you,
198,This is definitely one of the best hours I spent on YouTube. :),
199,Link to the home.html file: https://github.com/jimdevops19/codesnippets/tree/main/Python%20Web%20Scraping/01%20-%20Scraping%20Basics,
200,Amazing class really loved it,
201,"One question: every 10 min, wouldn't the files be overwritten since they'll still be named after the index (which starts with 0)?",
202,Thanks for the tutorial. Learned something new,
203,Thanks for such a great video!,
204,could you please share your html code so I can also use that one as a reference please?,
205,"Thanks a lot, it is really beautiful",
206,"Keep it up man, this just beautifull.",
207,Very informative about data scrapping.,
208,Your teaching is amazing man.,
209,"thanks for this awesome course but could you please check the link for the codes, it seems broken.",
210,"Thanks I learned a lot,Very Nice Presentation of python code.",
211,Great! Specially for automated running.,
212,Thank you for explaining so simple,
213,"question: to get the ""more_info"" (the job url), why use ""...job.header.h2.a['href']"",  instead of ""job.find()"" like on company and skills?",
214,Can someone please clarify when to use .find(sample_tag) function and when to use .sample_tag attribute ? Thanks in advance !,
215,Thank you so much. Great Tutorial!,
216,I have a question how do you know that the output will be printed out on a txt file when in the python code its only to index?,
217,"So Amazing! , what a great tutorial!",
218,Thank you so much. Valuable content,
219,good video.  I learned many interesting things. keep it up.,
220,"I really learned from ur tutorial 
Thank you so much for the good explanation",
221,Awesome tutorial for sure.,
222,"Thank you, you've helped me a lot",
223,Congrats man. Your eloquency is enough to teach anyone. Leave alone the content,
224,Great Explanation in a simple way,
225,"Thanks so much for the video. Really enlightening. 

47:00, don't you think the 'published date' should be before the for loop and not inside it?",
226,This was excellent!,
227,"Great tutorial, thank you !!",
228,Use text.strip() method to remove the white spaces at the beginning and ending of a string,
229,"This is code for unfamiliar skills:- This is code making filter for unfamiliar skills
If you have any efficient code please share!


print(""Give the skill that you are not familiar with"")
unfamiliar_skill = input()
unfm_s = unfamiliar_skill.split()
for job in jobs:
    flag = True
    published_date = job.find(""span"",class_=""sim-posted"").span.text
    if ""few"" in published_date:
        company_name = job.find(""h3"",class_=""joblist-comp-name"").text.replace("" "","""")
        skills = job.find('span',class_=""srp-skills"").text.replace("" "","""")
        link = job.header.h2.a['href']
        for i in unfm_s:
            if i not in skills:
                flag = True
            else:
                flag = False
                break
        if flag == True:
            print(f""""""Compnay Names: {company_name.strip()}\nSkills: {skills.strip()}\nLink:{link}"""""")
            print(""\n"")",
230,thank you for the tutorial is very simple to understood,
231,great video !! Thank you FCC and Jim,
232,Thank you so much for such a great video,
233,"Thanks for the content. Careful in using the ""if few"" statement. If a post is posted today, you get the ""posted today"" and this would be discarded by that ""if."" Apart from that great tutorial!",
234,Is it possible to add html file to work with used in this tutorial? Thank you,
235,it was really mind-blowing,
236,Amazingly Explained !!!,
237,"Awesome, thanks Jim !!",
238,"Very nice tutorial, thanks",
239,"Nice tutorial. Plus, very original way of pronouncing 'the'.",
240,i love the fact that you made python code to find a python job,
241,this is a nice refresher for someone out of touch with things like myself,
242,Absolutely Fantastic! subscribed to your channel. Thanks a bunch! @jimshapedcoding,
243,"This is great, thank you . what are interface is that? Is it an editor where you're working in? If so what editor?",
244,His teaching skills and accent are just fantastic.,
245,Thank you !!. Lots of love. Beautifully explained.,
246,How do you scrape the information from other pages on the website?,
247,"Amazing Content, Keep it Up",
248,"When the instructor prints the (jobs) the come out neatly organized in the terminal like you would see the source html code, but when I print out (jobs) they look so crammed with no indentations. Does anyone know how I can have similar results?",
249,"Hi,

I noticed that the replace() also eliminated the spaces within the name of the company. Is there any way to avoid that?

Thanks",
250,"An excellent illustration of using Python to scrape a web page. Thanks.

{2022-11-27}",
251,Do you have a video showing how we can write this type of data into a spreadsheet? Into particular columns etc.,
252,Please tell me the difference between select and find all method and which one to use when,
253,how do you extract the years of experience ? which is in the form of 2 - 3 yrs .. which is below the company name and beside the suitcase icon,
254,Will I be able to scrape e-commerce shopify websites using this? Like variants and all images in csv files?,
255,Thanks for this tutorial. I've i followed every step and have successfully scraped data from my chosen website. This is great content,
256,Great job man!!!,
257,"you can use .""strip"" method to delete unwanted white spaces it's much better than replace :).",
258,"thanks for putting this very useful tutorial up! 
anyone knows why was .header used instead of .find to scrape for 'more_info'? @52:53",
259,great job and great video : ),
260,what level of python knowladge do i need before doing this? IS there any videos that you could recommend me for it?,
261,"You know when Neo said "" I know ju-jitsu""? Now I can say the same....I know webscraping. :)) Thanks mate. You rock!!",
262,"the link to the code snippets seems broken, is it possible to fix it? Anyway, many many thanks for another great tutorial!",
263,Good Lecture ,
264,"Nice tutorial video, thank you.",
265,"sir you are really amazing,, .. your way of explaining is very easy,,,,,",
266,"Count von Count leaves Sesame Street to pursue programming dream 

Simple and thorough explanation ",
267,best 1h on youtube in my life. thanks for amazing video,
268,"The way he says, ""Excuse me,"" whenever he's fixing a variable name lol!! So polite

Excellent tutorial thank you!",
269,37:25 can we use strip() method to eliminate all the white spaces??,
270,"Just finished watching the video, thanks a lot bruh. It really helped a lot",
271,feeling satisfied after learning something..thank you free code camp,
272,"remember to ""import lxml"" before using",
273,The files are here: https://github.com/jimdevops19/codesnippets/tree/main/Python%20Web%20Scraping,
274,so tried to scrape 2-3 websites so far and all i get is empty lists of arrays when am searching for html 5 tags so i did some digging and found out that some websites are JavaScript generated so the tags you see on the inspect element are not really there idk if am getting this right if you try to print the html_text in pycharm you can see what i mean is a totally different code no tutorial for that i guess but nice job i learned a lot,
275,"So this is a question to anyone willing to answer. I'm stuck in the beggining where you are labeling the hmtl_tags with ""course"" infront. Where are you getting ""course"" in the html to make it ""course_html_tags"" or ""course_cards"" etc?",
276,"time stamp - 45:50
AttributeError: ResultSet object has no attribute 'find'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
277,"57:35 My 2 answers :
1) 
        familiar_skills=input('>')
        familiar_skills_list=familiar_skills.split(';')
        required_skills_list=required_skills.split(',')
        for i in required_skills_list:
            if i in familiar_skills_list :
                indicator+=1
        if indicator==len(required_skills_list):
            print.....

2) 
        familiar_skills=input('>')
        familiar_skills_list=familiar_skills.split(';')
        required_skills= job.find('span',class_='srp-skills').text.replace(' ','').split()
        required_skills_list=(required_skills[0]).split(',')
        check = all(item in familiar_skills_list for item in required_skills_list)
        if check is True:
            print.....",
278,Great start in learning Web Scraping,
279,"Thanks genius explaining, wow",
280,Yeah awesome information greatly appreciated.,
281,Amazing explanation..️️,
282,Useful bro️,
283,"thankyou very much, this is the best course that i watched ever",
284,what a great content!!,
285,Very good video lesson excellent explanation and didactic congratulations,
286,Thank you so much for your nice lecture.....,
287,Excellent bro hats off to you,
288,"When I run the code from 12:20 in the video, I get the error:

Traceback (most recent call last):
  File ""C:\Users\Tommy Duffy\PycharmProjects\helloWorld\main.py"", line 6, in <module>
    soup = BeautifulSoup(content, 'lxml')
  File ""C:\Users\Tommy Duffy\PycharmProjects\helloWorld\venv\lib\site-packages\bs4\_init__.py"", line 243, in __init_ 
    raise FeatureNotFound(
bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

I installed the lxml library with no problems. HELP ME",
289,"Really good video, thanks",
290,"I got the same results for job.find(""span"", class_=""sim-posted"").text.strip() and job.find(""span"", class_=""sim-posted"").find('span').text.strip() Any idea how? Does .text method just remove all the <html> tags?",
291,"This is what I need as I always working with data. 
How can I get in contact with you for mentorship? 
I will highly appreciate hearing from you.",
292,very good tutorial. unfortunatly it's not going to help me as im trying to scrape a java based website was worth a try I was hoping to not have to use visual studio I'm struggling with that and theres no decent tutorials for it :sigh:,
293,"This is what I need as I always working with data. 
How can I get in contact with you for mentorship? 
I will highly appreciate hearing from you.",
294,"damn that timing^^
Just two days ago i tried to do Web-Scraping with Python (to track PS5 availability) but it never worked bcs of captcha/recaptcha.
Hope this will help me.",
295,That is so great !,
296,Very silly question: Why do  we need python or any library at all to do web scraping? Doesn't vanilla js have all these methods like document.querySelectorAll?,
297,"Great video, thanks.",
298,"this is my first time, when I watched the entire video without yawning. it was super interesting!",
299,Is his html file downloadable so that we can follow his steps (in the  beginning) him as he codes?,
300,thank you so much for perfect course,
301,great session,
302,"Thanks, great video!",
303,"Have no background in this at all, watching this before I’m required to know it…THANK YOU",
304,Thanks a lot for your video. How i can download all your code?,
305,Thank you very much for this effort...please make a elixir tutorial video.,
306,I learned web scraping better here for free than my almost $1000 course at Uni,
307,Personal Timestamp: 18:30,
308,"thankyou , great explanation",
309,"Thank you for the tutorial it's really helpfull
I was trying to access the site but it is down showing a error 500.",
310,I'm from italy and i can say that your english is very understandable from a 19 years old boy. Thanks a lot.,
311,how would one go about broadening the ability of this program by allowing the user to input the job description that is being searched for,
312,"Jim: ""Now I will close back the head and then I will expand the body.""

Me: ...sorry I think I'm in the wrong class.",
313,Anyone know how I could use this to download all the videos from an online course I purchased?,
314,amazing tutorial,
315,"Very good lesson, greeting from 03/2022!",
316,Beautiful.,
317,i have a question : can we create keyword search tool with web scraping for ex: i want to know how many time a specific keyword was searched on the google searsh engine,
318,This is an amazing tutorial....,
319,Great tutorial,
320,This was great.,
321,Is web scrapping still effective since mostly html is automatically generate? Like react stuff,
322,Very informative video,
323,Would I need PyCharm proffessional to make use of these libraries or is the community edition enough?,
324,Perfect tutorial,
325,You guys are the best.,
326,"Awesome️️
We also need selenium",
327,Thanks for this I was wondering about it.,
328,Best tutorial of beautiful soup ever,
329,"many thanks, but i want to ask where is the home.html file i can downlaod for learning?",
330,"While using lxml, to parse, it says ""bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"", What is to be done",
331, love it,
332,Awesome that's what i'm looking for,
333,"Please don't scrape bank websites unless you have contacted the financial institution first.  They can be a paranoid bunch, and often have ToS that limit what can be accessed and how.",
334,Instead of using replace we can use stirng(),
335,"I guess if I don’t know what you do with this training, I don’t need to learn it. Lol! ...But curious. What are the top things one might do with webscraping?",
336,Thank you very much for your lesson,
337,Outstanding!!,
338,"This is unnecessarily big as a video, but it's way better than someone else that just told you how to click using selenium.",
339,When I used the find_all for loop only one element in the list returned. What is the reason?,
340,The best crush course.,
341,"from bs4 import BeautifulSoup

with open('home.html', 'r') as html_file:
    content = html_file.read()

    soup = BeautifulSoup(content, 'lxml')
    course_cards = soup.find_all('div', class_='card')
    for course in course_cards:
        course_name = course.h5.txt
        course_price = course.a.txt
        print(course_name)
        print(course_price)",
342,CAN ANYONE PLEASE SHARE THE HTML FILE THAT HE IS USED IN THIS VIDEO,
343,Joder debo aprender inglés de una vez por todas.,
344,great tutorial.,
345,Great english ,
346,Not the best tutorial for beginners. Luckily i loaded up multiple tutorials from YouTube with plans to watch them all to see who would teach better. I watched Corey Schafer's tutorial first and is much easier to understand since he uses HTML language to explain how to target what you want from the html. His workflow is also easier to understand. Still appreciate the effort here .,
347,Can you provide Github for this project?,
348,nice course as a begginer,
349,"I think replace() should be replaced with strip(). I mean, strip() works better @37:00.",
350,"Long term subscriber, a great tutorial. Does YouTube have a remind me feature?",
351,"Hello, I am from a non-tech background. I want to learn python for data scraping only.

Could you tell me on which website are typing python codes.
I tried doing this on pycharm but it id not working (however I feel that how can it connect my website or file, probably that's why)",
352,Is it possible if the website has multiple pages to scrap? And I want to get all of the data from all the pages,
353,hello can you please update the code link of the files,
354,"what is the python code to get text which is not in any div, id or class?",
355,"Hey guys,
IF YOU HAVEING A PROBLEM WITH PYCHARM AND YOUR LXML CODE 
IN 2022
Where you code Pareser you need to code 

features = ""lxml""


lol, I had the same problem and i was working on this for days and days
and they were no Youtube tutorials that were helping so it was very good to make that breakthrough
i think there was an update in Pycharm that changed the way the Parasers will be working ,but yea GoodLuck!",
356,Good job  ,
357,Can you give the link of the web that can be scrapped off legally?,
358,"Not able to get code snippets. Showing Error 404 not found. If you can provide the code snippets, it would be very helpful. Thanks",
359,"im using sublime text but what should I do with when I enter ubuntu to run the python script it tells me ""posts is not a file or directory""? I even used mkdir posts to see if that would work and it doesn't.
also im in the same working directory",
360,@41:07 I got everything just fine but I got a weird newline between Required Skills: and the skills when it printed. Any way to clean up newlines with text.replace?,
361,How do I import my html file into PyCharm?,
362,"Nice i am in love with python , c# and unity",
363,Great  video sir.,
364,Nice Work!,
365,1. Of the best programming vids i have see,
366,Where is the html code for the scrapping? On the website its not mentioned and not working,
367,thanks for the good work,
368,"Hi, does this Apply to intranet from a company?",
369,How much do webscrapers make and is there a big market for it ?,
370,thanks...am glad your video showed out the first when I searched,
371,"Is there a way to put all the results in text file, and not 5 billion text files?",
372,how can i get the html source code for the home.html you scraped,
373,what if there is no class name only table or id ?? what should I do in this case??,
374,Awesome Stuff!!! Thanks,
375,Can we scrap a page that need login first ?,
376,really thanks the content was helpful,
377,At last how to include all txts in one file rather than creating multiple text files?? please help,
378,What a LIFE SAVER..,
379,bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?,
380,37:40 It would be nicer if you used .strip() instead of .replace(),
381,"Hi, which platform did he using to run this code jupyter or colab",
382,Very nice explanation,
383,which software is he using to do this?,
384,it worked for scraping local html file but in real websites job is returned as an empty list or a nonetype what do i do,
385,53:51 what if I want to get the info inside that link as well? now just the link itself... thank you so much!,
386,Great course,
387,"sorry any chance we can have the .html file you re using at the beginning (the one with 'learn python for 20$'), thx",
388,"The website I'm trying to request doesn't give me all the information from the website,  and it changes its script over time, guess that's the problem, what should i do? however thanks for the video.",
389,How do you get the html file? I want to automatically check a website every few minutes.,
390,"My program keeps writing over the file, rather than creating a new one. Any suggestions?

if 'few' in published_date:
            company_name = job.find('h3', class_ = ""joblist-comp-name"").text.replace(' ', '')
            skills = job.find('span', class_ = 'srp-skills').text.replace(' ', '')
            more_info = job.header.h2.a['href']
            if unfamiliar_skill not in skills:
                with open(f'posts/{index}.txt', 'w') as f:
                    f.write(f'Company Name: {company_name.strip()} \n')
                    f.write(f'Required Skills: {skills.strip()} \n')
                    f.write(f'More Info: {more_info} \n')
                print(f'File saved: {index}')",
391,I want a link to the html source code,
392,Make a web scrapping using Java also,
393,please describe webscraping with oops concept,
394,I am trying to access the code snippets but receive a '404 not found' error. Could you please check?,
395,thank you it was useful,
396,"thanks, you just saved my time on 7:40",
397,"I have a problem, the requests library give captcha after some results :(",
398,Thank you so much ,
399,What IDE are you using here?,
400,"The link to acces the home.html file throws a 404 error, where can I find the file to work with it?",
401,The code snippets link doesn't work. Where else can I get the code?,
402,So I have a question how can I practice web scraping I mean like practising problem solving on code forces is there a site to practice web scraping ?,
403,How to scrape a website which requires login authentication?,
404,Please can u Explain if i wanna scrape details from ( more_info link)  in 54:15,
405,Wow you are an excellent teacher!!!!!!!!!!!!1,
406,Thank you!,
407,How can i login and do some scraping from a website with beautifulsoup ?,
408,"bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

FIXED THIS, I think I just implemented html.parser by putting ""pip install html.parser"" in the windows terminal and then I replace .lxml to html.parser",
409,where can you get the files you are using? Example home.html,
410,"i have a problem with the empty brackets, when i want to scrap a title of h2 or h3 tags or a text which is between span tags when i run the code i get nothing or empty brackets. 
can you help me ??",
411,"While scraping the timesjob html, jobs is not finding anything with the same class and returning None.
Any ideas?",
412,Which text editor was he using..?,
413,Thanks a bunch ,
414,I am lost in the first few minutes itself! Not sure how did you use home.html? Where do I get that file?,
415,Can it grab specific message from email? Im trying to make an excel bot,
416,nice tutorial bro,
417,could you scrap ads that have been shared and paid for on social media? it would be awesome if I can cut some costs on ads-spy tools etc.,
418,"@8:50 he passes the Home.html file to the open function, I am trying to follow along so where did he get this file from?",
419,Where can I find the code associated with this video? I checked the link in the description but can't find the code.,
420,but what if I want to search inside the website input things hit buttons?,
421,"I'm getting this error in PyCharm:
bs4.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?

I am trying to run this code: 
from bs4 import BeautifulSoup
with open('/Users/bazilzainal/Downloads/bs4test.html', 'r') as html_file:
    content = html_file.read()
    soup = BeautifulSoup(content, 'lxml')
    print(soup.prettify())


I have also ensured that the lxml package is installed in my environment. Would anyone know how to solve this please? Thanks in advance!",
422,Very Informative,
423,What should i learn before this video in addition to python basics??,
424,thank you very much brother,
425,Thank you. Спасибо.,
426,Whats the multi paging concept ??,
427,"At 33.40 when I print job, I have the output as ""[ ]"" . Why? how to fix this problem? I have done properly what u have done here.",
428,Awesome video,
429,thank you very much you are awesome,
430,"when I do print(Jobs) is just get closed brackets [] can anyone help?

Edit: When I soup.find_all I get the brackets, when I do soup.find I get none, still need help",
431,Thank you very much.,
432,"All is nice up until you try that on a real site and get blocked/banned because you're not able to hide the fact, that a script is trying to scrape a page. Then you get into the rabbit hole of chrome driver modification(s), full set of (user-agent) header rotation, javascript probing evasions, ip proxy rotation, cookies, captchas, behavioral patterns and way beyond that... Happy scraping then :-)",
433,"I cant use find after using find_all,  help please.",
434,if you have problem getting text with the number use  import re,
435,Can I know what is the name of this code editor?,
436,"the website used here shows potential security risk, should I proceed or try something myself",
437,Thank you for sharing,
438,"I am not sure how to run my code, it returns errors but I have everyting installed....  should I use the terminal? Or how can I see my code outpot? can some1 help me ?",
439,"33:53 for some pages, isometimes it won't show any script , and i read it's because of java on client side, im lost",
440,if I want to print html_text I always get <Response [200]> can someone help me with that?,
441,"my bs4 file is not being found, beautiful soup is installled and in site-packages folder. Anyone seen this before?",
442,Can anyone help me with getting the link for double underscore name = double underscore main. I am not seeing any link. Thanks!!,
443,"when I use find function to find anything. Showing an error "" 'NoneType' object has no attribute 'find' """,
444,What if it's a website in which I need to login to use? How should I use the request then?,
445,thank you!,
446,thank you !,
447,You have not shared anywhere this main.py code :(,
448,Thanks ,
449,"Commenting here for those who need the website right now. I am very sorry but I ran into some issues that forced me to take it down temporarily, everything will be back up by 18/1/2021.",
450,"30:36
useful very hekpful",
451,How to scrape data from age restricted videos?,
452,ModuleNotFoundError: No module named 'bs4' i keep getting this error,
453,how do i find the beautiful soup file,
454,ModuleNotFoundError: no module named ‘bs4’,
455,the text storing part doesn't work for me :(,
456,code snippets URL not found. Please provide the code.,
457,It's awesome :)))))))).,
458,"File ""c:\Users\Username\Documents\python\noni.py"", line 1, in <module>
    from bs4 import BeautifulSoup
ModuleNotFoundError: No module named 'bs4

What am I doing wrong :(",
459,Udemy scraping  will be my second project,
460,Thank you so much,
461,Amazing,
462,What about when the text is in <span,
463,thanks a lot!,
464,Really Good,
465,"Hi, I am not getting a 200 code or anything as a response to the request. could you help, please",
466,A request please do another crash course with SCRAPY,
467,Thank you so much,
468,What does lxml realy means? is like an html extension?,
469,Omg I was thinking just about this and you posted this. Are you guys some kind of wizard.,
470,Good job,
471,"I really could have used this 4 months ago....
I’m still gonna watch it though",
472,Thank you so much,
473,"So, your going to.... :))
Good lecture. Thanks !",
474,how can I get the job title ? and I want a way to delete the excessive spacing just one \n,
475,"İ got   an Error   AtributeError:""NoneType"" object has no attribute ""find"" i wrote  the same  code  but  i could not  get  scraping  element. in my  command  prompt or in  Idle. Please  inform me  İ  wil be  glad if so:::::",
476,"when i tried to install lxml it says 

 Could not find function xmlCheckVersion in library libxml2. Is libxml2 installed?

can anyone tell me solution step by step in windows 11",
477,good job,
478,it is possible to click on a button using Beautiful Soup?(a botton that has no link),
479,pls what interpreter are you using ?,
480,Can't extract company name. Error... find() can't have a keyword,
481,"I dont think the link after ""You can get code snippets here: "" works.",
482,What IDE is he using?,
483,I want to learn how to scrape through a website which requires login details,
484,Good one,
485,"Throws following error when I tried to access indeed.com.
Can anyone explain
requests.exceptions.ConnectionError: ('Connection aborted.', PermissionError(13, 'Permission denied'))",
486,"Hello Jim, can we connect once if you are free, I'm facing a lot of challenges, can we meet via gmeet?",
487,may I ask what is the app that you are using?,
488,u  kill it bro,
489,What editor are you using?,
490,"Is there a way I can get text from this tag, <span data-v-7372d7ee="""" class=""heading"">22°C</span> ?",
491,thank you so much,
492,sample code link is not working,
493,"love and respect from #India, thanks a lot.....",
494,please what IDE did you use here?,
495,good video,
496,Can anyone give me source code for this \The link is not working for codesnippets,
497,where can i get source code ????????,
498,"its not working for me 
apparently there is some error with lenght",
499,"I'm trying to scrape a youtube channel page and when I do print(content) I get ""UnicodeDecodeError: 'charmap' codec can't decode byte 0x8f in position 1069324: character maps to <undefined>"". help anyone?",
500,Which code runner application. He use?,
501,"It worked, the it just stop working, i didn't change anything, 

it went from show the result to saying none 

to saying [ ] 

to saying _init__.py"", line 310, in __init_ 
    elif len(markup) <= 256 and (
TypeError: object of type 'Response' has no len()


the file has no more than 15 lines, how is it reading line 310?",
502,"After changing the job into jobs it's showing find can't take arguments, can anyone help me out with this?",
503,"I’m getting error on installing chrome driver , can anyone help ?",
504,I've connected to my Google home using blue tooth and am listening the lesson there,
505,Source Code ?,
506,can you develop a Alzheimer's detection using Brain MRI as a challenge within 3 days?,
507,Can someone please sned the HTML file for this video. I am unable to open the website.,
508,Link for CODE SNIPPETS is not working. Please fix it.,
509,I finished :'),
510,"I got an error bs4 reportMissingModuleSource.
Please, anyone, solve this error.",
511,逻辑清晰,
512,why does the 'course.h5' format not work for me?,
513,"I can’t do it bc at the top the 
from bs4 import BeautifulSoup and the
import requests were both errors.",
514,very good,
515,Please share your code,
516,maybe share link from HTML code????,
517,1:03:36..........saving data into file,
518,Thank you,
519,Thank you,
520,Nice source code site is down :(,
521,I'm a Web Scraper Now:),
522,Thank  you,
523,Your teaching is good,
524,thanks got it,
525,"program text often blurry at 360P, have watched hundred of videos at 360P with clearer text. should not have to increase resolution and suck more bandwidth for this channel",
526,"He uses PYcharm and not VS code, off to a good start.....but then sees he uses windows and not MAC.....",
527,"I keep getting [SSL: CERTIFICATE_VERIFY_FAILED], 
please help I've literally tried everything (I have windows 10 installed).",
528,"Guys How can I solve ""with open('home.html', 'r')"" error 
Terminal shows me(No such file or directory: 'home.html')",
529,thank you,
530,I like this guy.,
531,"Hey guys, anyone figure out how to filter out the results for multiple unfamiliar skills? I got a mental block and unable to figure it out.. calling out for clues!! ref: 57:23",
532,"Unable to capture records name , price and rating and image in Requests Python
https://stackoverflow.com/q/66365160/8643009?sem=2",
533,he looks like bruno fernandes haha. But ngl this is and amazing course!!!!,
534,your website is down for the code snippets.,
535,"I cant acces the Web Page timesjobs.com , 403 Error , any Tips?",
536,I am getting NoneType error...,
537,"while True:
    print('Fantastic!!')",
538,,
539,helps,
540,My program is not working! Please tell me why?,
541,hey make a video how we will scrape timejobs website multiple page at onece plz plz plz,
542,Horse racing is what I want to crawl,
543,Web scraping with API,
544,Nice,
545,he gives me that Elliot-before-drugs vibe,
546,Nice,
547,Hello I'm going to create a search engine can you help me bro,
548,i m getting response 404 error ...how to resolve it?,
549,"FWIW, your website appears to be offline",
550,I'm so stupid I would use C++ to parse an HTML file when I could just as easily use python,
551,please upload video relate to adidas it denied the requests please help me?,
552,I dont understand y u wouldnt just .strip () insted of replace,
553,"thanks,dude,here we go again",
554,Is there anyone who added more than one unfamiliar skills?,
555,My teacher told me to do after this video as a homework and im lost..,
556,10.27 it's not opening what to do,
557,please upload video on Alexarankchecker,
558,<3,
559,I was fortunate to do 1000th like on this video,
560,"""lxml doesn't parse broken html well""... proceeds to use it with his own html sigma male 101, king
37:13 why no .strip() here? :D",
561,When has a car crash ever taken 1 hour 8 minutes.. this is a course not a crash course,
562,Please i would like to know if Web Scraping is legal ?,
563,I am confused what I learn in python,
564,"When I run the program it shows this:

None
None
None
None
None
None

Process finished with exit code 0",
565,Free  code camp on time,
566,"i can't get it to work it always returns ""none"" or ""[]""",
567,At first I thought the accent would be annoying but now I need more of you just talking lol,
568,what if the list has no class?,
569,the code source is not working,
570,"omg, i love your accent haha
where are you from?",
571,thanks,
572,thanks,
573,Freecodecamp please make video on playwright,
574,Hi sir business website PTC ads and create now,
575,unfamiliar skill condition isn't working,
576,37:14 that didn't solve the issue,
577,"great tutorial, the only complaint is the accent, i mean seriously he pronounces python pytchon",
578,Good,
579,57:45,
580,27:27 yes two years,
581,Can Anyone Suggest best laptop for coding?,
582,"When i run the code i get an empty list ,if printed, [] <--- this",
583,25:00,
584,"when every 5 seconds he says „going to“, it becomes hard to focus. ",
585,the most usfuel video I had watched,
586,30:00,
587,sadly the site i was trying to use this on has some kind of robot capcha so im fucked,
588,"regelRDW=rdwBestand.readline()
while regelRDW != """":
    lijstRDW.append(regelRDW.split(','))
    regelRDW=rdwBestand.readline()
 
21 regelScan=scanBestand.readline()
while regelScan != """":
    lijstScan.append(regelScan.split(','))
    regelScan=scanBestand.readline()
 
22  for scan in lijstScan:
    kentekenScan = scan[0].strip()
    gevonden=False
 
23   for rdw in lijstRDW:
        kentekenRDW = rdw[0].strip()
 
24/26    if kentekenScan == kentekenRDW:
            gevonden=True
            print(f""OK: kenteken {kentekenScan} is in orde."")
            kleurRDW = rdw[4].strip()
 
            kleurNaam = """"
            for list in kleuren:
                if list[0] == kleurRDW:
                    kleurNaam = list[1]
        
27/29     if kleurNaam != """":
                if kleurNaam ==  scan[3].strip():
                    print(f""OK: Kleur voor {kentekenScan} komt overeen."")
                else:
                    print(f""FOUT: Kleur voor {kentekenScan} komt niet overeen."")
            else:
                print(f""FOUT: Er kon geen kleur worden gevonden voor {kleurRDW}"")
            break
 
    if not gevonden:
        print(""FOUT: Kenteken kon niet worden gevonden in de database van RDW, is dit een Nederlandse auto?"")",
589,bruh jobstimes is forbidden in germany xd why coudlnt u just take a normall webside :D,
590,How much news you scrap in this video,
591,21:31,
592,Thabks from Algeria,
593,Please how can I post as a guest here,
594,13:26,
595,Am I the only one getting a FileNotFoundError??,
596,Greaat,
597,└ good s,
598,His look is a lot like Ryan's from The Office,
599,the borat voice makes this easier to digest,
600,too much commercials,
601,you sound like you have a Russian accent,
602,Frecodecamp zo'r Uzbekustondan,
603,His accent getting worse as he is getting more tired :DDD,
604,you look like Bruno Fernandes,
605,"Python is cute, and its so fuckin neat that it doesnt have the fuckin semicolons...hated them!",
606,So far the HTML explanation is not that good,
607,Oh! my! god! scrapy does it better,
608,hey are you turkish?,
609,the accent is heavvvy,
610,"hahaha accent lolololol
",
611,Your accent sounds middle east,
612,hi,
613,"The greedy moat bareilly mate because quilt morphometrically work since a free george. low, strong person",
614,"awful accent, barely saw the video because of that",
615,Literally a waste of my time,
616,Borat teaches Python!,
617,L,
618,I stopped watching because of the accent,
619,Please learn to pronounce 'THE' !  Gave up watching 'THE' video at min. 10:00.,
